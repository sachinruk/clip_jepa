{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db325596",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543c949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from src import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3bcafc810342a78ebce8f18e54d21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7f014fabad4fa490c67032925afab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00010-45de7542ea7caa(…):   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928e6af0287498cbb00656bd4ea2d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00010-941dfd963f3fb7(…):   0%|          | 0.00/487M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bfb14c8fc84d298a9b13aea1d754f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00010-584b44bfff9f35(…):   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe28d48e6634e20ba8ad77ffef618bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00010-c4cd69058db789(…):   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccf017d732e483482816a1966d29ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00010-6ac76be6e02176(…):   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b64e4939054e079b3d0241cfa30875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00010-82089d811e0f58(…):   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2b0aba27a548e3ae6e7f2a69449a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00010-d6594693b4c0e5(…):   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e7e14c057e4a37a94b696fbc1ecf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00010-e2c13ff1928c45(…):   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959a613491664e31b3891cfc24e33830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00010-dbfd724640b186(…):   0%|          | 0.00/487M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb55304c8534ce0b5fafe61e1a1bf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00010-06eda47bab140d(…):   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5803bd2dc243a2b61db8604943281d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyper_parameters = config.HyperParameters()\n",
    "\n",
    "dataset: datasets.Dataset = datasets.load_dataset(\n",
    "    hyper_parameters.data_config.dataset, split=\"train\"\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20ac050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x428>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>],\n",
       " 'caption': ['A group of zebras grazing in the grass.',\n",
       "  'a number of people standing around a large group of luggage bags',\n",
       "  'A yellow commuter train traveling past some houses.']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6446fa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6cbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fadba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def collate_fn(batch: list[dict[str, Image.Image | str]]) -> dict[str, list[list[dict[str, Any]]]]:\n",
    "    images = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": data_instance[\"image\"],\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for data_instance in batch\n",
    "    ]\n",
    "    texts = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": data_instance[\"caption\"]}],\n",
    "            }\n",
    "        ]\n",
    "        for data_instance in batch\n",
    "    ]\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"texts\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "dl = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f753112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f0b6a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "isinstance(dataset, datasets.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e042a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "148d3784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [[{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x428>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=550x365>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=428x640>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=450x450>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x469>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x332>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427>}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426>}]}]],\n",
       " 'texts': [[{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Several different zebras standing in the tall grass.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Some people that are eating some food together.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'People are standing in the grass near flying kites.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Three young zebras standing in a Savannah plain. '}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Hand holding a hotdog with another on a blue plate.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'two women in a kitchen bottles and lights'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'a baseball player that is at home plate'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Three stuffed bears hugging and sitting on a blue pillow'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'This is the image if a bed with black and white cover'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'An intersection with cars and buildings and a tree. '}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'A guy standing by a couple of very nice motorcycles.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'A zebra standing next to another rolling in the dirt.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'A person sitting in the snow with a snowboard on.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'A couple of white ranges, sitting alongside the wall.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Three cheese pizza sticks are on a plate.'}]}],\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'A snowy expanse covers an area outside of a small area with hay, penned in with stakes and barbed wire, that also houses a large evergreen,providing shelter to a large huddle of big sheep. '}]}]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e87642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data, config, model\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d3ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-05 18:09:39.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data\u001b[0m:\u001b[36mget_dataset\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mLoading Dataset...\u001b[0m\n",
      "Using the latest cached version of the dataset since sayakpaul/coco-30-val-2014 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/sachinthaka/.cache/huggingface/datasets/sayakpaul___coco-30-val-2014/default/0.0.0/abdde3200f533ddae5bed2438057f1f7ea2d5131 (last modified on Sun Oct  5 12:15:53 2025).\n",
      "\u001b[32m2025-10-05 18:09:43.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data\u001b[0m:\u001b[36mget_dataset\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mSplitting dataset\u001b[0m\n",
      "\u001b[32m2025-10-05 18:09:43.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data\u001b[0m:\u001b[36m_get_dataloaders\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mCreating Dataloaders\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = config.HyperParameters()\n",
    "train_dl, valid_dl = data.get_dataset(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7468a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82a07aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d8b16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be582f9f931246e2a822ed740410af13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jepa_model = model.CLIPJepaModel(hyper_parameters.llm_model_config, device=torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97eff27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'texts'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64b16e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_embeddings = jepa_model.embed_text(batch[\"texts\"])\n",
    "    # image_embeddings = jepa_model.embed_image(batch[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf6fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2048]),\n",
       " torch.Size([8, 2048]),\n",
       " tensor(8., device='mps:0'),\n",
       " tensor(8., device='mps:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    text_embeddings.shape,\n",
    "    image_embeddings.shape,\n",
    "    (text_embeddings**2).sum(),\n",
    "    (image_embeddings**2).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc5bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
