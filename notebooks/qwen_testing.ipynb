{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b17f972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273b04536381415fbc68e1195a828c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5612c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts a serene beach scene at sunset. A person is sitting on the sandy shore, facing a large, light-colored dog that appears to be a Labrador Retriever. The dog is sitting on its hind legs and is giving the person a high-five with its front paws. The person is wearing a plaid shirt and black pants, and they are smiling warmly. The background shows the ocean with gentle waves lapping against the shore, and the sky is filled with warm hues of orange and yellow from the setting sun. The overall atmosphere is peaceful and joyful, capturing a moment of connection between the person and their dog.']\n"
     ]
    }
   ],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", max_pixels=768 * 768)\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"mps\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9bfe31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 751]),\n",
       " 'attention_mask': torch.Size([1, 751]),\n",
       " 'pixel_values': torch.Size([2904, 1176]),\n",
       " 'image_grid_thw': torch.Size([1, 3])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fc9b71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(726, device='mps:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs[\"input_ids\"][0] == 151655).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f20ac8",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d425e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"</EMBED>\" in processor.tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1136d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<PIL.Image.Image image mode=RGB size=2044x1372>], None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_inputs, video_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d70376dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "074cebb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151667, 2048)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": [\n",
    "            \"<EMBED>\",\n",
    "            \"</EMBED>\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "model.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ea09e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151665, 151666)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_start_token_id = processor.tokenizer.convert_tokens_to_ids(\"<EMBED>\")\n",
    "embed_end_token_id = processor.tokenizer.convert_tokens_to_ids(\"</EMBED>\")\n",
    "embed_start_token_id, embed_end_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d7f7616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|><|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d60240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[f\"<EMBED>{text_line}</EMBED>\" for text_line in text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd5ea79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 749]),\n",
       " 'attention_mask': torch.Size([1, 749]),\n",
       " 'pixel_values': torch.Size([2904, 1176]),\n",
       " 'image_grid_thw': torch.Size([1, 3])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "716335d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([151665, 151644,   8948,    198,   2610,    525,    264,  10950,  17847,\n",
       "            13, 151645,    198, 151644,    872,    198, 151652, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "        151655, 151655, 151655, 151655, 151653, 151645,    198, 151644,  77091,\n",
       "           198, 151666], device='mps:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ff252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97869b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aebeaafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.hidden_states[-1][inputs[\"input_ids\"] == embed_end_token_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1eeb50a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 749, 2048])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b2c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|><|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d256b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1915371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"qkv\", \"fc1\", \"fc2\", \"linear\", \"proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "lora_model = peft.get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d629658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.visual.patch_embed.proj.base_layer.weight torch.Size([1280, 3, 2, 14, 14])\n",
      "model.visual.patch_embed.proj.lora_A.default.weight torch.Size([32, 3, 2, 14, 14])\n",
      "model.visual.patch_embed.proj.lora_B.default.weight torch.Size([1280, 32, 1, 1, 1])\n",
      "model.visual.patch_embed.proj.lora_magnitude_vector.default.weight torch.Size([1, 1280, 1, 1, 1])\n",
      "model.visual.blocks.0.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.0.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.0.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.0.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.0.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.0.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.0.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.0.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.0.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.0.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.0.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.0.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.0.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.0.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.0.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.0.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.0.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.0.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.1.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.1.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.1.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.1.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.1.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.1.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.1.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.1.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.1.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.1.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.1.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.1.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.1.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.1.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.1.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.1.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.1.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.1.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.2.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.2.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.2.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.2.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.2.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.2.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.2.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.2.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.2.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.2.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.2.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.2.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.2.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.2.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.2.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.2.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.2.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.2.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.3.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.3.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.3.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.3.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.3.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.3.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.3.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.3.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.3.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.3.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.3.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.3.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.3.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.3.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.3.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.3.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.3.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.3.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.4.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.4.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.4.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.4.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.4.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.4.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.4.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.4.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.4.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.4.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.4.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.4.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.4.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.4.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.4.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.4.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.4.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.4.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.5.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.5.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.5.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.5.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.5.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.5.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.5.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.5.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.5.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.5.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.5.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.5.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.5.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.5.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.5.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.5.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.5.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.5.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.6.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.6.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.6.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.6.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.6.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.6.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.6.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.6.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.6.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.6.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.6.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.6.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.6.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.6.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.6.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.6.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.6.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.6.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.7.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.7.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.7.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.7.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.7.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.7.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.7.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.7.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.7.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.7.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.7.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.7.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.7.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.7.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.7.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.7.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.7.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.7.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.8.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.8.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.8.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.8.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.8.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.8.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.8.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.8.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.8.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.8.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.8.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.8.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.8.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.8.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.8.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.8.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.8.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.8.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.9.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.9.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.9.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.9.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.9.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.9.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.9.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.9.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.9.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.9.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.9.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.9.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.9.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.9.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.9.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.9.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.9.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.9.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.10.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.10.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.10.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.10.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.10.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.10.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.10.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.10.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.10.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.10.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.10.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.10.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.10.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.10.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.10.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.10.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.10.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.10.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.11.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.11.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.11.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.11.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.11.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.11.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.11.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.11.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.11.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.11.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.11.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.11.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.11.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.11.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.11.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.11.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.11.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.11.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.12.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.12.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.12.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.12.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.12.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.12.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.12.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.12.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.12.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.12.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.12.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.12.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.12.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.12.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.12.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.12.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.12.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.12.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.13.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.13.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.13.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.13.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.13.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.13.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.13.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.13.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.13.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.13.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.13.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.13.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.13.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.13.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.13.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.13.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.13.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.13.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.14.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.14.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.14.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.14.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.14.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.14.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.14.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.14.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.14.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.14.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.14.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.14.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.14.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.14.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.14.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.14.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.14.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.14.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.15.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.15.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.15.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.15.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.15.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.15.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.15.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.15.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.15.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.15.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.15.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.15.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.15.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.15.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.15.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.15.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.15.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.15.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.16.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.16.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.16.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.16.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.16.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.16.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.16.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.16.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.16.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.16.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.16.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.16.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.16.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.16.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.16.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.16.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.16.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.16.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.17.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.17.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.17.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.17.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.17.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.17.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.17.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.17.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.17.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.17.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.17.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.17.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.17.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.17.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.17.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.17.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.17.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.17.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.18.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.18.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.18.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.18.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.18.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.18.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.18.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.18.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.18.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.18.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.18.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.18.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.18.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.18.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.18.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.18.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.18.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.18.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.19.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.19.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.19.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.19.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.19.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.19.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.19.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.19.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.19.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.19.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.19.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.19.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.19.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.19.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.19.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.19.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.19.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.19.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.20.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.20.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.20.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.20.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.20.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.20.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.20.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.20.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.20.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.20.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.20.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.20.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.20.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.20.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.20.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.20.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.20.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.20.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.21.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.21.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.21.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.21.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.21.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.21.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.21.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.21.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.21.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.21.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.21.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.21.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.21.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.21.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.21.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.21.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.21.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.21.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.22.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.22.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.22.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.22.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.22.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.22.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.22.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.22.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.22.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.22.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.22.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.22.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.22.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.22.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.22.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.22.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.22.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.22.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.23.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.23.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.23.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.23.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.23.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.23.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.23.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.23.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.23.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.23.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.23.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.23.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.23.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.23.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.23.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.23.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.23.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.23.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.24.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.24.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.24.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.24.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.24.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.24.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.24.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.24.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.24.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.24.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.24.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.24.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.24.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.24.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.24.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.24.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.24.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.24.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.25.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.25.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.25.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.25.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.25.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.25.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.25.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.25.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.25.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.25.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.25.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.25.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.25.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.25.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.25.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.25.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.25.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.25.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.26.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.26.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.26.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.26.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.26.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.26.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.26.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.26.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.26.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.26.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.26.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.26.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.26.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.26.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.26.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.26.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.26.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.26.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.27.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.27.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.27.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.27.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.27.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.27.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.27.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.27.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.27.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.27.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.27.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.27.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.27.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.27.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.27.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.27.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.27.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.27.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.28.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.28.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.28.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.28.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.28.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.28.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.28.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.28.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.28.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.28.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.28.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.28.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.28.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.28.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.28.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.28.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.28.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.28.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.29.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.29.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.29.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.29.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.29.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.29.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.29.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.29.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.29.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.29.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.29.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.29.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.29.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.29.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.29.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.29.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.29.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.29.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.30.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.30.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.30.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.30.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.30.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.30.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.30.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.30.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.30.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.30.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.30.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.30.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.30.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.30.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.30.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.30.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.30.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.30.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.blocks.31.norm1.weight torch.Size([1280])\n",
      "model.visual.blocks.31.norm2.weight torch.Size([1280])\n",
      "model.visual.blocks.31.attn.qkv.base_layer.weight torch.Size([3840, 1280])\n",
      "model.visual.blocks.31.attn.qkv.base_layer.bias torch.Size([3840])\n",
      "model.visual.blocks.31.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.31.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "model.visual.blocks.31.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "model.visual.blocks.31.attn.proj.base_layer.weight torch.Size([1280, 1280])\n",
      "model.visual.blocks.31.attn.proj.base_layer.bias torch.Size([1280])\n",
      "model.visual.blocks.31.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "model.visual.blocks.31.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "model.visual.blocks.31.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "model.visual.blocks.31.mlp.gate_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.31.mlp.gate_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.31.mlp.up_proj.weight torch.Size([3420, 1280])\n",
      "model.visual.blocks.31.mlp.up_proj.bias torch.Size([3420])\n",
      "model.visual.blocks.31.mlp.down_proj.weight torch.Size([1280, 3420])\n",
      "model.visual.blocks.31.mlp.down_proj.bias torch.Size([1280])\n",
      "model.visual.merger.ln_q.weight torch.Size([1280])\n",
      "model.visual.merger.mlp.0.weight torch.Size([5120, 5120])\n",
      "model.visual.merger.mlp.0.bias torch.Size([5120])\n",
      "model.visual.merger.mlp.2.weight torch.Size([2048, 5120])\n",
      "model.visual.merger.mlp.2.bias torch.Size([2048])\n",
      "model.language_model.embed_tokens.weight torch.Size([151667, 2048])\n",
      "model.language_model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.0.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.0.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.0.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.0.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.0.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.0.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.0.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.0.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.1.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.1.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.1.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.1.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.1.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.1.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.1.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.1.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.2.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.2.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.2.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.2.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.2.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.2.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.2.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.2.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.3.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.3.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.3.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.3.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.3.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.3.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.3.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.3.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.4.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.4.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.4.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.4.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.4.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.4.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.4.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.4.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.5.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.5.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.5.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.5.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.5.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.5.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.5.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.5.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.6.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.6.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.6.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.6.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.6.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.6.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.6.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.6.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.7.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.7.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.7.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.7.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.7.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.7.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.7.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.7.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.8.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.8.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.8.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.8.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.8.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.8.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.8.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.8.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.9.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.9.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.9.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.9.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.9.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.9.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.9.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.9.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.10.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.10.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.10.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.10.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.10.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.10.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.10.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.10.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.11.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.11.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.11.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.11.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.11.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.11.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.11.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.11.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.12.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.12.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.12.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.12.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.12.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.12.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.12.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.12.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.13.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.13.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.13.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.13.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.13.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.13.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.13.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.13.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.14.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.14.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.14.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.14.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.14.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.14.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.14.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.14.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.15.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.15.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.15.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.15.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.15.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.15.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.15.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.15.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.16.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.16.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.16.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.16.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.16.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.16.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.16.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.16.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.16.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.16.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.16.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.17.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.17.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.17.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.17.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.17.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.17.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.17.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.17.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.17.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.17.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.17.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.18.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.18.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.18.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.18.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.18.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.18.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.18.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.18.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.18.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.18.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.18.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.18.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.19.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.19.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.19.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.19.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.19.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.19.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.19.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.19.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.19.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.19.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.19.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.19.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.20.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.20.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.20.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.20.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.20.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.20.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.20.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.20.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.20.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.20.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.20.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.20.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.21.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.21.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.21.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.21.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.21.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.21.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.21.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.21.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.21.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.21.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.21.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.21.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.22.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.22.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.22.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.22.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.22.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.22.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.22.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.22.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.22.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.22.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.22.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.22.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.23.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.23.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.23.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.23.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.23.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.23.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.23.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.23.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.23.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.23.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.23.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.23.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.24.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.24.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.24.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.24.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.24.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.24.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.24.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.24.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.24.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.24.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.24.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.24.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.25.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.25.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.25.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.25.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.25.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.25.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.25.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.25.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.25.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.25.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.25.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.25.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.26.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.26.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.26.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.26.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.26.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.26.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.26.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.26.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.26.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.26.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.26.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.26.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.27.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.27.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.27.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.27.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.27.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.27.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.27.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.27.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.27.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.27.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.27.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.27.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.28.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.28.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.28.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.28.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.28.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.28.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.28.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.28.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.28.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.28.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.28.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.28.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.29.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.29.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.29.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.29.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.29.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.29.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.29.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.29.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.29.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.29.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.29.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.29.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.30.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.30.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.30.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.30.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.30.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.30.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.30.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.30.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.30.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.30.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.30.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.30.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.31.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.31.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.31.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.31.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.31.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.31.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.31.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.31.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.31.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.31.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.31.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.31.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.32.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.32.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.32.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.32.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.32.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.32.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.32.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.32.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.32.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.32.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.32.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.32.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.33.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.33.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.33.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.33.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.33.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.33.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.33.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.33.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.33.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.33.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.33.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.33.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.34.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.34.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.34.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.34.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.34.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.34.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.34.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.34.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.34.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.34.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.34.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.34.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.35.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.35.self_attn.q_proj.bias torch.Size([2048])\n",
      "model.language_model.layers.35.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.35.self_attn.k_proj.bias torch.Size([256])\n",
      "model.language_model.layers.35.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.language_model.layers.35.self_attn.v_proj.bias torch.Size([256])\n",
      "model.language_model.layers.35.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.language_model.layers.35.mlp.gate_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.35.mlp.up_proj.weight torch.Size([11008, 2048])\n",
      "model.language_model.layers.35.mlp.down_proj.weight torch.Size([2048, 11008])\n",
      "model.language_model.layers.35.input_layernorm.weight torch.Size([2048])\n",
      "model.language_model.layers.35.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.language_model.norm.weight torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "034b92c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.visual.patch_embed.proj.lora_A.default.weight torch.Size([32, 3, 2, 14, 14])\n",
      "base_model.model.model.visual.patch_embed.proj.lora_B.default.weight torch.Size([1280, 32, 1, 1, 1])\n",
      "base_model.model.model.visual.patch_embed.proj.lora_magnitude_vector.default.weight torch.Size([1, 1280, 1, 1, 1])\n",
      "base_model.model.model.visual.blocks.0.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.0.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.0.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.0.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.0.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.0.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.1.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.1.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.1.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.1.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.1.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.1.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.2.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.2.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.2.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.2.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.2.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.2.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.3.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.3.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.3.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.3.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.3.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.3.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.4.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.4.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.4.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.4.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.4.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.4.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.5.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.5.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.5.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.5.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.5.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.5.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.6.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.6.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.6.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.6.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.6.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.6.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.7.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.7.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.7.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.7.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.7.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.7.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.8.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.8.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.8.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.8.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.8.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.8.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.9.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.9.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.9.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.9.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.9.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.9.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.10.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.10.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.10.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.10.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.10.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.10.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.11.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.11.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.11.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.11.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.11.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.11.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.12.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.12.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.12.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.12.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.12.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.12.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.13.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.13.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.13.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.13.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.13.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.13.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.14.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.14.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.14.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.14.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.14.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.14.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.15.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.15.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.15.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.15.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.15.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.15.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.16.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.16.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.16.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.16.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.16.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.16.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.17.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.17.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.17.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.17.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.17.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.17.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.18.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.18.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.18.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.18.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.18.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.18.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.19.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.19.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.19.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.19.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.19.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.19.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.20.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.20.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.20.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.20.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.20.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.20.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.21.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.21.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.21.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.21.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.21.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.21.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.22.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.22.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.22.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.22.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.22.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.22.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.23.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.23.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.23.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.23.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.23.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.23.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.24.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.24.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.24.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.24.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.24.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.24.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.25.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.25.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.25.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.25.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.25.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.25.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.26.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.26.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.26.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.26.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.26.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.26.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.27.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.27.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.27.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.27.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.27.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.27.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.28.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.28.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.28.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.28.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.28.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.28.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.29.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.29.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.29.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.29.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.29.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.29.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.30.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.30.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.30.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.30.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.30.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.30.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n",
      "base_model.model.model.visual.blocks.31.attn.qkv.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.31.attn.qkv.lora_B.default.weight torch.Size([3840, 32])\n",
      "base_model.model.model.visual.blocks.31.attn.qkv.lora_magnitude_vector.default.weight torch.Size([3840])\n",
      "base_model.model.model.visual.blocks.31.attn.proj.lora_A.default.weight torch.Size([32, 1280])\n",
      "base_model.model.model.visual.blocks.31.attn.proj.lora_B.default.weight torch.Size([1280, 32])\n",
      "base_model.model.model.visual.blocks.31.attn.proj.lora_magnitude_vector.default.weight torch.Size([1280])\n"
     ]
    }
   ],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3048f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04 17:07:28.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mTrainable portion: 0.0014, trainable params: 5365760\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainable_params, all_params = lora_model.get_nb_trainable_parameters()\n",
    "logger.info(f\"Trainable portion: {trainable_params / all_params:.4f}, trainable params: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c4a14232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([p for p in lora_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab42942f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([p for n, p in lora_model.named_parameters() if \"lora\" in n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fdac1313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('base_model.model.model.language_model.embed_tokens.weight',\n",
       "  torch.Size([151667, 2048]),\n",
       "  False)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, p.shape, p.requires_grad) for n, p in lora_model.named_parameters() if \"embed_tokens\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c64bf915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/sachinthaka/personal_work/clip_jepa/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"qkv\", \"fc1\", \"fc2\", \"linear\", \"proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    ")\n",
    "lora_model = peft.get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0cd24100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('base_model.model.lm_head.modules_to_save.default.weight',\n",
       "  torch.Size([151667, 2048]),\n",
       "  True)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, p.shape, p.requires_grad) for n, p in lora_model.named_parameters() if \"lm_head\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5121878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.language_model.embed_tokens.original_module.weight',\n",
       "  torch.Size([151667, 2048]),\n",
       "  False),\n",
       " ('model.language_model.embed_tokens.modules_to_save.default.weight',\n",
       "  torch.Size([151667, 2048]),\n",
       "  True)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, p.shape, p.requires_grad) for n, p in model.named_parameters() if \"embed_tokens\" in n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7d9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
